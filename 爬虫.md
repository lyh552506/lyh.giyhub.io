# 爬虫

## 一.爬虫学习笔记

#### 1.定义

​      通过编写程序，模拟浏览器上网，然后让其去互联网抓取数据的过程

​       可分为：善意爬虫，恶意爬虫

####  2.注意事项（如何避免入局子）

1.时常优化自己的程序，避免干扰被访问网站的正常运行

2.在使用，传播爬取到的数据时，审查抓取到的内容

####  3.爬虫分类

##### 1.通用爬虫

抓取系统重要组成部分，抓取的是一整张页面数据{**浏览器工作原理**}

##### 2.聚焦爬虫

建立在通用爬虫的基础之上，抓取的是页面中特定的局部内容

##### 3.增量式爬虫

检查网站中数据更新的情况。只会抓取网站中最新更新出来的数据

**##robots.txt协议：**

君子协议，可以不遵从，规定了网站哪些数据可以被爬虫爬取

**（如何查看一个网站的robot协议？）**：在网站后缀/robots.txt

#### 4.初始化设备

##### 1.下载pycharm和python

![](https://s1.ax1x.com/2022/09/07/vH4Hkd.png)

![](https://s1.ax1x.com/2022/09/07/vH4TTH.png)

##### 2.配置python环境

将下载好的pycharm配置好python环境

点击file-settings来到界面

![](https://s1.ax1x.com/2022/09/07/vH5sjf.png)

进入python interpreter界面

![](https://s1.ax1x.com/2022/09/07/vH5Ig0.png)

点击add interpreter，找到自己安装的python位置并进行配置

![](https://s1.ax1x.com/2022/09/07/vH5OUJ.png)

##### 3.安装requests

![](https://s1.ax1x.com/2022/09/07/vHIPbD.png)

#### 5.使用方法

###### 1.指定url

###### 2.发送请求

###### 3.获取响应数据

###### 4.持久化存储

#### 6.小练手--爬取搜狗页面源代码

###### 在搜狗页面取得的源代码

![](https://s1.ax1x.com/2022/09/07/vH76Rs.png)

###### pycharm输入代码

![](https://s1.ax1x.com/2022/09/07/vH75oF.png)

###### ##其中：

url='https://m.sogou.com/'(爬取到搜狗网页)

response=requests.get(获取响应数据，并用response命令获得)

###### 结果：

![](https://s1.ax1x.com/2022/09/07/vHHwlR.png)

#### 7.UA伪装

##### 为何需要？

门户网站的服务器会检查对应请求的载体身份标识，如检查到请求的载体身份标识为某一款浏览器，说明那个是一个正常请求。相反若检查到的身份不正常，则服务器可能拒绝该请求。

##### 查看方法

在该网页右击点击检查，在Network一项找到自己的user-agent

![](https://s1.ax1x.com/2022/09/07/vHjJTx.png)

##### 使用方法

将对应的请求载体身份标识伪装成某一浏览器

添加”headers = {

​           ’User-Agent'：Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36

#### 8.实验

##### firsy：使用chrome浏览器点开链接

![](https://s1.ax1x.com/2022/09/11/vOyq6U.png)

随后点击鼠标右键打开检查，在其中的network选项中可以看到

![](https://s1.ax1x.com/2022/09/11/vO6St1.png)

有一行行任务名出现（商品信息，图片，参数内容……），在此任务名中需要找到我们需要的评论

![](https://s1.ax1x.com/2022/09/11/vO6Nhq.png)

可以看到这是该网站的静态部分{.html}

但是该界面鱼龙混杂，很难找到我们需要的评论，所以我通过搜索关键字的方法缩小我们的范围

![](https://s1.ax1x.com/2022/09/11/vO6LCt.png)

可以看到出现了comments,继续点击进入后评论信息便出现在了框内

![](https://s1.ax1x.com/2022/09/11/vORA2t.png)

可以得到评论是动态的数据，根据搜索得知动态数据一般在JS中，所以我将搜索范围锁定在JS中并将之前数据清空，翻到下一页

![](https://s1.ax1x.com/2022/09/12/vXlBN9.png)

可以快速得到这一页的评论数据

##### sec：知识盲区的普及

1：字典：一种数据类型，是许多值的集合，字典的定义为“{}”，基本格式为

字典名字 = {键:值，键:值.....,键:值，键:值}，中间用逗号隔开

2：获取到的是一些类似jason的动态数据直接转换成字典进行处理；获取到的是一些静态的hdml类似的数据，我们采用Xpath或CSS选择器进行处理

3.<class'str'>含义：变量是字符串类型

4.python的循环结构—for_in:for x in y+循环体，x依次表示y中的一个元素，遍历完所有元素循环结束

##### third：解析数据

![](https://s1.ax1x.com/2022/09/12/vXfNJH.png)

![](https://s1.ax1x.com/2022/09/12/vXftFe.png)

可以看到callback和response都存在相同的fetchjson_comment98,

将获取到的request url在另一页面打开：

![](https://s1.ax1x.com/2022/09/12/vXfowT.png)

这就是评论的最初始模样，接下来我们需要筛选其中的评论部分，剔除其他的无用信息

##### fouth:构建python模板

在pycharm中创建一个新的文件，导入requests，并指定url为上述我们看到的request url，并且在Request Method上可以看到是GET， 所以根据python语法格式（？）写入requests.get(url)

![](https://s1.ax1x.com/2022/09/12/vXhvCQ.png)

运行后能够看到爬取结果包含有评论

![](https://s1.ax1x.com/2022/09/12/vX4e29.png)

但我们的目标是只收集评论，并且对应各自的id号与评分等

于是我在后缀中加上打印出response.text的命令：print(type(response.text))

运行后找到了该形式为：<class 'str'>

说明该变量是字符串，而这一个字符串的开头和结尾含有[]或{}则根据资料的查找和学习可以得知这是一个json格式的字符串

![](https://s1.ax1x.com/2022/09/13/vj7uIx.png)

将json格式的字符串转化为字典，才能将字典内的文字评论提取出来（目前暂不知道其原理）

在最上方引入json，并在下方输入json.loads

![](https://s1.ax1x.com/2022/09/13/vj7ces.png)

将json格式的字符串转化为字典

![](https://s1.ax1x.com/2022/09/13/vj7TOJ.png)

此时运行程序能够看到我们的type已经变成了<class'dict'>***{详见知识盲区普及}***

*![](https://s1.ax1x.com/2022/09/13/vj7qT1.png)*

##### fifth:解析数据

在这里采用for_in的循环结构，依次循环得到我们需要的nickname，id，score，creationtime

在最后将取得数据中的\n用空格进行替换，采用replace（‘’，‘’）的格式

![](https://s1.ax1x.com/2022/09/15/vxu4KK.png)

##### sixth：如何实现评论翻页？

###### 1将代码封装成一个类——作用？？

封装的含义：封装是实现面向对象程序设计的第一步，封装就是将数据或函数等集合在一个个的单元中（我们称之为类）。被封装的对象通常被称为抽象数据类型。

封装的作用：封装应该是把一些功能通过函数和变量集合到一个对象上面，别的程序可以通过接口来调用这个封装的对象里面的功能。

###### 2class()和class(object)【我也不太明白】

当括号内有object时class就继承了object的属性，在下方可以定义一些属性

###### 3def_init_(self)

由于类起到模板的作用，因此，可以在创建实例的时候，把认为必须绑定的属性强制填写进去。这里就用到Python当中的一个内置方法`__init__`

class下方def_init_(self),self是指向该对象本身的一个引用，通过在类的内部使用self变量，类中的方法可以访问自己的成员变量，我们在创建类实例时都想要做些初始化操作，为此类定义时可以定义一个名为def_init_()的特殊方法

###### 4输入代码

首先在class下定义一个初始化的方法def_init_(self),并且在初始化的下方加入headers代理网址>>(初始化init里的内容定义后可能是作为这个爬虫的内在属性，只需要定义一次，后面翻页时用到的headers都是这个初始化里的headers)，随后再定义一个解析一页的方法，**因为要用到翻页，所以在class类中再定义一个翻页方法**

![](https://s1.ax1x.com/2022/09/15/vzAB5t.png)



```python
def parse_one_page(self):
    pass
def parse_max_page(self):
    pass
```

即从第一页爬到最后一页

###### 5设置爬虫打开文件

运用语法self.fp，同样文件只需要打开一次记录后续爬取的内容，所以我们在初始化里打开一个文件

```python
self.fp=open('./jingdong_pinglun.txt',)
```

ps**:fp这个变量接受到的是open的返回值**

​      fp函数的使用格式：**fp =open("文件名",mode="采用的模式",encoding="使用什么编码集")**

mode：![](https://s1.ax1x.com/2022/09/25/xEc2Vg.png)

​       所以加入代码

```python
self.fp=open('./jingdong_pinglun.txt','w',encoding='utf-8')

```

也可def_init括号里写入

```python
def __init__(self,file_name='jingdong_pinglun'):
```

同时

```python
self.fp=open(f'./{file_name}.txt','w',encoding='utf-8')
```

补充：括号里的f意义？

f-string，亦称为格式化字符串常量，形式上是以 f或 F修饰符引领的字符串（`f'xxx'` 或 `F'xxx'）以大括号{}标明被替换的字段，此处则是

以"write"（只读）形式打开它，采用编码形式utf-8（不太了解）

并且在下面打印爬虫进度

```python
print(f'爬虫开始，打开{file_name}文件')
```

###### 6数据的存储

在第一页爬取的下面进行数据的存储

```python
self.fp.write(f'{nickname}\t{goods_id}\t{score}\t{creationTime}\t{content}\t')
```

补充学习：**write()** 方法用于向文件中写入指定字符串，将所需要的昵称，商品id等等以字符串的形式写入初始化中的fp打开文件内,**因为之前我们写入需要txt形式，所以在每一个{}后加上\t。**

###### 7实现翻页

在def parse_num_page(self):下采用for in的循环结构实现翻页

```python
for page_num in range(101):
    print(f'正在抓取{page_num}页的数据')
```

通过京东查取可以得到评论最多有 100页，通过循环的结构，在循环体里导入目标网站的url

```python
url=f‘https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98&productId=100019125569&score=0&sortType=5&page=1&pageSize=10&isShadowSku=0&rid=0&fold=1’
```

可以看到在url中有目标网站评论区的页数

![](https://s1.ax1x.com/2022/10/13/xdZHO0.png)

这里是第一页的评论，**要实现循环只需改变page=1这一数据,让page={page_num},即可实现翻页循环**

随后进行函数调用【作用是我推测的】

```python
self.parse_one_page(url=url)
```

作用：在上一节parse_one_page后加入了url

![](https://s1.ax1x.com/2022/10/13/xdm8Dx.png)

故在此处调用url并赋以循环的变量值

###### 8爬虫结束后关闭文件

利用def函数，定义关闭文件方法

【def函数语法：def  函数名  （参数列表）：】

```python
def close_files(self):
    self.fp.close()
    print('爬虫结束，关闭文件')
```





